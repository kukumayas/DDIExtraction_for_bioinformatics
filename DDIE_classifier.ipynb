{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#数据部分\n",
    "import numpy as np\n",
    "import os, nltk\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "ddi_db_2013_train_xml_folder = 'xmlformat/Train2013/DrugBank'\n",
    "ddi_db_2013_test_xml_folder = 'xmlformat/Test2013/DrugBank'\n",
    "ddi_ml_2013_train_xml_folder = 'xmlformat/Train2013/MedLine'\n",
    "ddi_ml_2013_test_xml_folder = 'xmlformat/Test2013/MedLine'\n",
    "\n",
    "ddi_2011_train_xml_folder = 'xmlformat/Train2011'\n",
    "ddi_2011_test_xml_folder = 'xmlformat/Test2011'\n",
    "\n",
    "EMBEDDING_DIM = 400\n",
    "\n",
    "ddi_2013_all_train = '2013_all_train.txt'\n",
    "ddi_2013_all_train = '2013_all_train.txt'\n",
    "ddi_2013_DB_train = '2013_DB_train.txt'\n",
    "ddi_2013_ML_train = '2013_ML_train.txt'\n",
    "ddi_2013_DB_test = '2013_DB_test.txt'\n",
    "ddi_2013_ML_test = '2013_ML_test.txt'\n",
    "\n",
    "ddi_2011_train = '2011_train.txt'\n",
    "ddi_2011_test = '2011_test.txt'\n",
    "\n",
    "ddi_2011_sentences = '2011_sentences.txt'\n",
    "ddi_2013_sentences = '2013_sentences.txt'\n",
    "\n",
    "ddi_2011_gold = 'gold annotations test.txt'\n",
    "\n",
    "pad = 'NULL'\n",
    "r = 3\n",
    "\n",
    "special_chars_13 = [';', '!', '?', ',', ';', '*', '\\\\', '$', '%', '`', '~', '@', '^', '&', ':', '\"', '<', '>']\n",
    "special_chars_11 = [';', '!', '?', ',', ';', '*', '\\\\', '$', '%', '`', '~', '@', '^', '&', ':', '\"', '<', '>', '/', '(',\n",
    "                    ')', '[', ']', '-']\n",
    "\n",
    "DDIE_2011_gold_standard = {}\n",
    "\n",
    "\n",
    "###\n",
    "### 数据预处理部分\n",
    "###\n",
    "\n",
    "def get_ranged_indices(str_indexs):\n",
    "    if str_indexs[0] == '[':\n",
    "        str_indexs = str_indexs[1:]\n",
    "    elif str_indexs.startswith('range('):\n",
    "        str_indexs = str_indexs[6:]\n",
    "    if str_indexs[-1] == ']' or str_indexs[-1] == ')':\n",
    "        str_indexs = str_indexs[:-1]\n",
    "    return [int(i.strip()) for i in str_indexs.split(',')]\n",
    "\n",
    "\n",
    "def read_corpus_new(text_file):\n",
    "    instances = []\n",
    "    for line in open(text_file):\n",
    "        instance = line.strip().split('\\t')\n",
    "        if len(instance) == 4:\n",
    "            target, sentence, d1i, d2i = instance\n",
    "            instances.append((target, sentence, get_ranged_indices(d1i), get_ranged_indices(d2i)))\n",
    "        elif len(instance) == 5:\n",
    "            target, sentence, d1i, d2i, pairid = instance\n",
    "            instances.append((target, sentence, get_ranged_indices(d1i), get_ranged_indices(d2i), pairid))\n",
    "    return instances\n",
    "\n",
    "\n",
    "def get_entity_by_offset(sentence, offset, plus_one=True):\n",
    "    if type(offset[0]) == tuple:\n",
    "        origin_entity = ' '.join([sentence[x[0]:x[1] + (1 if plus_one else 0)] for x in offset])\n",
    "        return origin_entity\n",
    "    else:\n",
    "        origin_entity = sentence[offset[0]:offset[1] + (1 if plus_one else 0)]\n",
    "        return origin_entity\n",
    "\n",
    "\n",
    "def get_entity_by_range(sentence, r):\n",
    "    return ' '.join(sentence.split(' ')[i] for i in r)\n",
    "\n",
    "\n",
    "def count_pair(e1, e2, counter, target, identical_test):\n",
    "    e1 = e1.replace(' ', '').replace('-', '')\n",
    "    e2 = e2.replace(' ', '').replace('-', '')\n",
    "    key1 = e1 + '_' + e2\n",
    "    key2 = e2 + '_' + e1\n",
    "    if key1 in counter:\n",
    "        counter[key1] += 1\n",
    "        if identical_test[key1] != target:\n",
    "            print(e1, e2, \"have different labels\")\n",
    "        return key1\n",
    "    elif key2 in counter:\n",
    "        counter[key2] += 1\n",
    "        if identical_test[key2] != target:\n",
    "            print(e1, e2, \"have different labels\")\n",
    "        return key2\n",
    "    else:\n",
    "        counter[key1] = 1\n",
    "        identical_test[key1] = target\n",
    "        return key1\n",
    "\n",
    "\n",
    "def label21hot(label, dict):\n",
    "    vector = [0] * len(dict)\n",
    "    vector[dict[label]] = 1\n",
    "    return vector\n",
    "\n",
    "\n",
    "def get_distance(idx, indices):\n",
    "    dis = [idx - x for x in indices]\n",
    "    return dis[np.argmin(np.abs(dis))]\n",
    "\n",
    "\n",
    "def summarize_ddi_instance(corpora_files=(ddi_2013_DB_train, ddi_2013_ML_train, ddi_2013_DB_test, ddi_2013_ML_test)):\n",
    "    DB_train_set = read_corpus_new(corpora_files[0])\n",
    "    ML_train_set = read_corpus_new(corpora_files[1])\n",
    "    DB_test_set = read_corpus_new(corpora_files[2])\n",
    "    ML_test_set = read_corpus_new(corpora_files[3])\n",
    "\n",
    "    train_set = DB_train_set + ML_train_set\n",
    "    test_set = DB_test_set + ML_test_set\n",
    "\n",
    "    train_counter = {}\n",
    "    test_counter = {}\n",
    "\n",
    "    identical_test = {}\n",
    "\n",
    "    train_ddi_list = []\n",
    "    test_ddi_list = []\n",
    "\n",
    "    for target, sentence, r1, r2 in train_set:\n",
    "        e1 = get_entity_by_range(sentence, r1)\n",
    "        e2 = get_entity_by_range(sentence, r2)\n",
    "        train_ddi_list.append(count_pair(e1, e2, train_counter, target, identical_test))\n",
    "        if e1 == 'BREVIBLOC' and e2 == 'digoxin':\n",
    "            print(sentence)\n",
    "\n",
    "    for target, sentence, r1, r2 in test_set:\n",
    "        e1 = get_entity_by_range(sentence, r1)\n",
    "        e2 = get_entity_by_range(sentence, r2)\n",
    "        test_ddi_list.append(count_pair(e1, e2, test_counter, target, identical_test))\n",
    "        if e1 == 'BREVIBLOC' and e2 == 'digoxin':\n",
    "            print(sentence)\n",
    "    exit(0)\n",
    "    import operator\n",
    "    print(sorted(train_counter.items(), key=operator.itemgetter(1)))\n",
    "\n",
    "    print(sum(train_counter.values()), list(train_counter.values()).count(1))\n",
    "    print(sorted(test_counter.items(), key=operator.itemgetter(1)))\n",
    "    print(sum(test_counter.values()), list(test_counter.values()).count(1))\n",
    "    print(max(train_counter.values()), max(test_counter.values()))\n",
    "    fd = nltk.FreqDist(train_ddi_list)\n",
    "    fd.plot(30, cumulative=False)\n",
    "    fd = nltk.FreqDist(test_ddi_list)\n",
    "    fd.plot(30, cumulative=False)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    #############\n",
    "    # LOAD DATA #\n",
    "    #############\n",
    "    lower_case = False\n",
    "    train_counter = {}\n",
    "    test_counter = {}\n",
    "    target_indices2013 = {}\n",
    "    target_indices2011 = {}\n",
    "\n",
    "    DB_train_set2013 = read_corpus_new(ddi_2013_DB_train)\n",
    "    ML_train_set2013 = read_corpus_new(ddi_2013_ML_train)\n",
    "    DB_test_set2013 = read_corpus_new(ddi_2013_DB_test)\n",
    "    ML_test_set2013 = read_corpus_new(ddi_2013_ML_test)\n",
    "    train_set2013 = DB_train_set2013 + ML_train_set2013\n",
    "    test_set2013 = DB_test_set2013 + ML_test_set2013\n",
    "\n",
    "    train_set2011 = read_corpus_new(ddi_2011_train)\n",
    "    test_set2011 = read_corpus_new(ddi_2011_test)\n",
    "\n",
    "    target_indices2013['-1'] = 0\n",
    "    target_indices2013['mechanism'] = 1\n",
    "    target_indices2013['effect'] = 2\n",
    "    target_indices2013['advise'] = 3\n",
    "    target_indices2013['int'] = 4\n",
    "\n",
    "    target_indices2011['0'] = 0\n",
    "    target_indices2011['1'] = 1\n",
    "\n",
    "    word_indices = {}  # word:index\n",
    "    word_indices[pad] = 0\n",
    "    dis_indices = {}\n",
    "    word_index = 1\n",
    "    dis_index = 1\n",
    "    for _, sentence, e1r, e2r in train_set2011 + test_set2011 + train_set2013 + test_set2013:\n",
    "        token_seq = sentence.split(' ')\n",
    "        for i, word in enumerate(token_seq):\n",
    "            if lower_case:\n",
    "                word = word.lower()\n",
    "            d1 = get_distance(i, e1r)\n",
    "            d2 = get_distance(i, e2r)\n",
    "            if d1 not in dis_indices:\n",
    "                dis_indices[d1] = dis_index\n",
    "                dis_index += 1\n",
    "            if d2 not in dis_indices:\n",
    "                dis_indices[d2] = dis_index\n",
    "                dis_index += 1\n",
    "            if word not in word_indices:\n",
    "                word_indices[word] = word_index\n",
    "                word_index += 1\n",
    "\n",
    "    def get_input_format(train_set, test_set, target_indices):\n",
    "        X_train_text = []\n",
    "        X_train_inner_tokens = []\n",
    "        X_train_surrounding_tokens = []\n",
    "        X_train_e1 = []\n",
    "        X_train_e2 = []\n",
    "        X_train_distance1 = []\n",
    "        X_train_distance2 = []\n",
    "        X_train_skeleton = []\n",
    "        y_train = []\n",
    "        X_test_text = []\n",
    "        X_test_inner_tokens = []\n",
    "        X_test_surrounding_tokens = []\n",
    "        X_test_e1 = []\n",
    "        X_test_e2 = []\n",
    "        X_test_distance1 = []\n",
    "        X_test_distance2 = []\n",
    "        X_test_skeleton = []\n",
    "        y_test = []\n",
    "        for target, sentence, e1r, e2r in train_set:\n",
    "            token_seq = sentence.split(' ')\n",
    "            X_train_text.append([word_indices[word.lower() if lower_case else word] for word in token_seq])\n",
    "            X_train_distance1.append([dis_indices[get_distance(i, e1r)] for i in range(len(token_seq))])\n",
    "            X_train_distance2.append([dis_indices[get_distance(i, e2r)] for i in range(len(token_seq))])\n",
    "            y_train.append(label21hot(target, target_indices))\n",
    "\n",
    "            skeleton_cache = [0] * len(token_seq)\n",
    "            cache = []\n",
    "            for i in range(max(e1r) + 1, min(e2r)):\n",
    "                cache.append(word_indices[token_seq[i].lower() if lower_case else token_seq[i]])\n",
    "                skeleton_cache[i] = 1\n",
    "            if len(X_train_inner_tokens) == 0:\n",
    "                cache.append(word_indices[pad])\n",
    "            X_train_inner_tokens.append(cache)\n",
    "\n",
    "            cache = []\n",
    "            for i in e1r:\n",
    "                cache.append(word_indices[token_seq[i].lower() if lower_case else token_seq[i]])\n",
    "                skeleton_cache[i] = 2\n",
    "            X_train_e1.append(cache)\n",
    "\n",
    "            cache = []\n",
    "            for i in e2r:\n",
    "                cache.append(word_indices[token_seq[i].lower() if lower_case else token_seq[i]])\n",
    "                skeleton_cache[i] = 3\n",
    "            X_train_e2.append(cache)\n",
    "\n",
    "            cache = []\n",
    "            for i in list(range(min(e1r) - r + 1, min(e1r))) + list(range(max(e1r) + 1, max(e1r) + r)) + list(range(\n",
    "                                    min(e2r) - r + 1, min(e2r))) + list(range(max(e2r) + 1, max(e2r) + r)):\n",
    "                if i < 0 or i > len(token_seq) - 1:\n",
    "                    cache.append(word_indices[pad])\n",
    "                else:\n",
    "                    cache.append(word_indices[token_seq[i].lower() if lower_case else token_seq[i]])\n",
    "                    if skeleton_cache[i] != 0:\n",
    "                        skeleton_cache[i] = 4\n",
    "                    else:\n",
    "                        skeleton_cache[i] = 5\n",
    "            X_train_skeleton.append(skeleton_cache)\n",
    "            X_train_surrounding_tokens.append(cache)\n",
    "\n",
    "        for target, sentence, e1r, e2r in test_set:\n",
    "            token_seq = sentence.split(' ')\n",
    "            X_test_text.append([word_indices[word.lower() if lower_case else word] for word in token_seq])\n",
    "            X_test_distance1.append([dis_indices[get_distance(i, e1r)] for i in range(len(token_seq))])\n",
    "            X_test_distance2.append([dis_indices[get_distance(i, e2r)] for i in range(len(token_seq))])\n",
    "            y_test.append(label21hot(target, target_indices))\n",
    "\n",
    "            skeleton_cache = [0] * len(token_seq)\n",
    "            cache = []\n",
    "            for i in range(max(e1r) + 1, min(e2r)):\n",
    "                cache.append(word_indices[token_seq[i].lower() if lower_case else token_seq[i]])\n",
    "                skeleton_cache[i] = 1\n",
    "            if len(X_test_inner_tokens) == 0:\n",
    "                cache.append(word_indices[pad])\n",
    "            X_test_inner_tokens.append(cache)\n",
    "\n",
    "            cache = []\n",
    "            for i in e1r:\n",
    "                cache.append(word_indices[token_seq[i].lower() if lower_case else token_seq[i]])\n",
    "                skeleton_cache[i] = 2\n",
    "            X_test_e1.append(cache)\n",
    "\n",
    "            cache = []\n",
    "            for i in e2r:\n",
    "                cache.append(word_indices[token_seq[i].lower() if lower_case else token_seq[i]])\n",
    "                skeleton_cache[i] = 3\n",
    "            X_test_e2.append(cache)\n",
    "\n",
    "            cache = []\n",
    "            for i in list(range(min(e1r) - r + 1, min(e1r))) + list(range(max(e1r) + 1, max(e1r) + r)) + list(range(\n",
    "                                    min(e2r) - r + 1, min(e2r))) + list(range(max(e2r) + 1, max(e2r) + r)):\n",
    "                if i < 0 or i > len(token_seq) - 1:\n",
    "                    cache.append(word_indices[pad])\n",
    "                else:\n",
    "                    cache.append(word_indices[token_seq[i].lower() if lower_case else token_seq[i]])\n",
    "                    if skeleton_cache[i] != 0:\n",
    "                        skeleton_cache[i] = 4\n",
    "                    else:\n",
    "                        skeleton_cache[i] = 5\n",
    "            X_test_skeleton.append(skeleton_cache)\n",
    "            X_test_surrounding_tokens.append(cache)\n",
    "\n",
    "        X_train_text = np.asarray(X_train_text)\n",
    "        X_train_inner_tokens = np.asarray(X_train_inner_tokens)\n",
    "        X_train_surrounding_tokens = np.asarray(X_train_surrounding_tokens)\n",
    "        X_train_e1 = np.asarray(X_train_e1)\n",
    "        X_train_e2 = np.asarray(X_train_e2)\n",
    "        X_train_distance1 = np.asarray(X_train_distance1)\n",
    "        X_train_distance2 = np.asarray(X_train_distance2)\n",
    "        X_train_skeleton = np.asarray(X_train_skeleton)\n",
    "        y_train = np.asarray(y_train)\n",
    "\n",
    "        X_test_text = np.asarray(X_test_text)\n",
    "        X_test_inner_tokens = np.asarray(X_train_inner_tokens)\n",
    "        X_test_surrounding_tokens = np.asarray(X_train_surrounding_tokens)\n",
    "        X_test_e1 = np.asarray(X_train_e1)\n",
    "        X_test_e2 = np.asarray(X_train_e2)\n",
    "        X_test_distance1 = np.asarray(X_test_distance1)\n",
    "        X_test_distance2 = np.asarray(X_test_distance2)\n",
    "        X_test_skeleton = np.asarray(X_test_skeleton)\n",
    "        y_test = np.asarray(y_test)\n",
    "\n",
    "        return [X_train_text, X_train_distance1, X_train_distance2, X_train_skeleton, X_train_inner_tokens,\n",
    "                X_train_surrounding_tokens,\n",
    "                X_train_e1, X_train_e2], y_train, [X_test_text, X_test_distance1,\n",
    "                                                   X_test_distance2, X_test_skeleton, X_test_inner_tokens,\n",
    "                                                   X_test_surrounding_tokens,\n",
    "                                                   X_test_e1, X_test_e2], y_test\n",
    "\n",
    "    return get_input_format(train_set2011, test_set2011, target_indices2011), get_input_format(train_set2013, test_set2013, target_indices2013), len(\n",
    "        word_indices), len(dis_indices)\n",
    "\n",
    "\n",
    "# 13的offset是卡到最后一个字符所在的位置，而11是卡到最后一个字符的下一个位置\n",
    "def decodeOffset(offset, version='13'):\n",
    "    if ';' in offset:\n",
    "        return tuple([decodeOffset(x) for x in offset.split(';')])\n",
    "    elif ',' in offset:\n",
    "        return tuple([decodeOffset(x) for x in offset.split(',')])\n",
    "    rawOffsets = offset.split('-')\n",
    "    return (int(rawOffsets[0]), int(rawOffsets[1]) if version == '13' else int(rawOffsets[1]) - 1)\n",
    "\n",
    "\n",
    "def getIndex(sentence, offset):\n",
    "    if type(offset[0]) == type(offset[1]) == tuple:\n",
    "        r = []\n",
    "        for tup in offset:\n",
    "            r += getIndex(sentence, tup)\n",
    "        return r\n",
    "    start = sentence[:offset[0]].count(' ')\n",
    "    end = sentence[offset[0]:offset[1]].count(' ') + start\n",
    "    return list(range(start, end + 1))\n",
    "\n",
    "\n",
    "# 单纯的把xml的内容解析成实例，不做任何预处理\n",
    "def get_instances_from_xml_file(xml_file, with_pairid=False, version='13'):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    instances = []\n",
    "\n",
    "    for child in root:\n",
    "        # print child.tag,\n",
    "        sentence = child.attrib['text']\n",
    "        entities = dict()\n",
    "        pairs = []\n",
    "        sub_instances = []\n",
    "        for grandson in child:\n",
    "            if grandson.tag == 'entity':\n",
    "                entities[grandson.attrib['id']] = (grandson.attrib['text'], grandson.attrib['charOffset'])\n",
    "            elif grandson.tag == 'pair':\n",
    "                pairid = grandson.attrib['id']\n",
    "                tag = ''\n",
    "                if version == '13':\n",
    "                    tag = grandson.attrib['type'] if grandson.attrib['ddi'] == 'true' else '-1'\n",
    "                elif version == '11':\n",
    "                    tag = grandson.attrib['interaction']\n",
    "                    tag = '1' if tag == 'true' else '0'\n",
    "                    if (grandson.attrib['e1'], grandson.attrib['e2']) in DDIE_2011_gold_standard:\n",
    "                        tag = DDIE_2011_gold_standard[(grandson.attrib['e1'], grandson.attrib['e2'])]\n",
    "                pairs.append((grandson.attrib['e1'], grandson.attrib['e2'], tag, pairid))\n",
    "\n",
    "        sentence = sentence.replace('\\n', ' ')\n",
    "        for e1id, e2id, target, pairid in pairs:\n",
    "            e1offset = decodeOffset(entities[e1id][1], version=version)\n",
    "            e2offset = decodeOffset(entities[e2id][1], version=version)\n",
    "\n",
    "            if with_pairid:\n",
    "                sub_instances.append((target, sentence, e1offset, e2offset, pairid))\n",
    "            else:\n",
    "                sub_instances.append((target, sentence, e1offset, e2offset))\n",
    "        instances.extend(sub_instances)\n",
    "    return instances\n",
    "\n",
    "\n",
    "def extract_sentence(corpus_files, sentence_file):\n",
    "    sentences = set()\n",
    "    for corpus in corpus_files:\n",
    "        for _, sentence, _, _ in read_corpus_new(corpus):\n",
    "            sentences.add(sentence)\n",
    "\n",
    "    writer = open(sentence_file, 'w')\n",
    "    for sentence in sentences:\n",
    "        writer.write(sentence + '\\n')\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def getNewOffset(origin_doc, new_doc, origin_offset, minus1=False):\n",
    "    while origin_doc[-1] == ' ':\n",
    "        origin_doc = origin_doc[:-1]\n",
    "    while new_doc[-1] == ' ':\n",
    "        new_doc = new_doc[:-1]\n",
    "\n",
    "    if type(origin_offset[0]) == type(origin_offset[1]) == tuple:\n",
    "        return tuple([getNewOffset(origin_doc, new_doc, offset) for offset in origin_offset])\n",
    "    # 游标i卡在origin上，游标j卡在new上\n",
    "    i = 0\n",
    "    j = 0\n",
    "    if minus1:\n",
    "        origin_offset = origin_offset[0], origin_offset[1] - 1\n",
    "    new_offset_start, new_offset_end = origin_offset[0], -1\n",
    "    # i和j的字符必须相同 此时i++ j++  检查i如果是start/end offset 记录j为new start/end offset\n",
    "    a = origin_doc.lower()\n",
    "    b = new_doc.lower()\n",
    "    lena = len(a)\n",
    "    lenb = len(b)\n",
    "    while i < lena and j < lenb:\n",
    "\n",
    "        # print a[i], b[j], '\\t', i, j\n",
    "        if a[i] == b[j]:\n",
    "            if i == origin_offset[0]:\n",
    "                new_offset_start = j\n",
    "            if i == origin_offset[1]:\n",
    "                new_offset_end = j\n",
    "\n",
    "            i += 1\n",
    "            j += 1\n",
    "            continue\n",
    "        elif a[i] == ' ':\n",
    "            i += 1\n",
    "        elif b[j] == ' ':\n",
    "            j += 1\n",
    "        else:\n",
    "            print(origin_doc)\n",
    "            print(new_doc)\n",
    "            print(u'输入的两个句子不是经过extreme tokenization算法得到的，退出')\n",
    "            return\n",
    "    if new_offset_end == -1:\n",
    "        new_offset_end = len(b) - 1\n",
    "    return new_offset_start, ((new_offset_end + 1) if minus1 else new_offset_end)\n",
    "\n",
    "\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "\n",
    "# 把没用的药物实体替换成字符串'drug'\n",
    "def replace_irrelavent_drugs(instances):\n",
    "    new_instances = []\n",
    "    sentence_dic = {}\n",
    "    for t, sentence, e1, e2 in instances:\n",
    "        if sentence not in sentence_dic:\n",
    "            sentence_dic[sentence] = [(e1, e2, t)]\n",
    "        else:\n",
    "            sentence_dic[sentence].append((e1, e2, t))\n",
    "    for sentence in sentence_dic:\n",
    "        entity_list = []\n",
    "        for e1, e2, _ in sentence_dic[sentence]:\n",
    "            if e1 not in entity_list:\n",
    "                entity_list.append(e1)\n",
    "            if e2 not in entity_list:\n",
    "                entity_list.append(e2)\n",
    "        for e1, e2, t in sentence_dic[sentence]:\n",
    "            origin_e1 = get_entity_by_range(sentence, e1)\n",
    "            origin_e2 = get_entity_by_range(sentence, e2)\n",
    "            token_list = sentence.split(' ')\n",
    "            for e in entity_list:\n",
    "                # 直接替换为@\n",
    "                for offset in e:\n",
    "                    if offset not in e1 and offset not in e2:\n",
    "                        token_list[offset] = '@' * len(token_list[offset])\n",
    "            new_sent = ' '.join(token_list)\n",
    "            matches = list(re.finditer(r\"@+[ @+]+@\", new_sent))\n",
    "            matches.reverse()\n",
    "            for match in matches:\n",
    "                match_span = match.span()\n",
    "                match_range = getIndex(sentence, match_span)\n",
    "                merges_no = new_sent[match_span[0]: match_span[1]].count(' ')\n",
    "\n",
    "                e1 = [x - merges_no if x > match_range[-1] else x for x in e1]\n",
    "                e2 = [x - merges_no if x > match_range[-1] else x for x in e2]\n",
    "                new_sent = new_sent[:match_span[0]] + 'drug' + new_sent[match_span[1]:]\n",
    "            new_instances.append((t, new_sent, e1, e2))\n",
    "            new_e1 = get_entity_by_range(new_sent, e1)\n",
    "            new_e2 = get_entity_by_range(new_sent, e2)\n",
    "            assert (origin_e1 == new_e1)\n",
    "            assert (origin_e2 == new_e2)\n",
    "\n",
    "    return new_instances\n",
    "\n",
    "\n",
    "def extreme_fast_tokenize_sentence(sentence, special_chars):\n",
    "    newSent = []\n",
    "    for token in sentence.strip().split(' '):\n",
    "        newToken = ''\n",
    "        for i in range(len(token)):\n",
    "            cc = token[i]\n",
    "            lc = ''\n",
    "            if len(newToken) != 0:\n",
    "                lc = newToken[-1]\n",
    "            if cc in special_chars:\n",
    "                if i > 0 and lc != ' ':\n",
    "                    newToken += ' '\n",
    "                newToken += token[i]\n",
    "                continue\n",
    "            if lc in special_chars and cc != ' ':\n",
    "                newToken += ' '\n",
    "            newToken += token[i]\n",
    "\n",
    "        def rm_bracket(newToken):\n",
    "            newToken = newToken.strip()\n",
    "            if len(newToken) > 0:\n",
    "                if newToken[0] == '(' and ')' not in newToken:\n",
    "                    newToken = newToken[0] + ' ' + rm_bracket(newToken[1:])\n",
    "                elif newToken[-1] == ')' and '(' not in newToken:\n",
    "                    newToken = rm_bracket(newToken[:-1]) + ' ' + newToken[-1]\n",
    "                elif newToken[0] == '(' and newToken[-1] == ')':\n",
    "                    newToken = newToken[0] + ' ' + rm_bracket(newToken[1:-1]) + ' ' + newToken[-1]\n",
    "                elif newToken[0] == '\\'' and newToken[-1] == '\\'':\n",
    "                    newToken = newToken[0] + ' ' + rm_bracket(newToken[1:-1]) + ' ' + newToken[-1]\n",
    "            return newToken\n",
    "\n",
    "        if newToken.count('.') == 1 or len(newToken) > 9:\n",
    "            newnewToken = ''\n",
    "            for i in range(len(newToken)):\n",
    "                cc = newToken[i]\n",
    "                lc = ''\n",
    "                if len(newnewToken) != 0:\n",
    "                    lc = newnewToken[-1]\n",
    "                if cc == '.':\n",
    "                    if i > 0 and lc != ' ':\n",
    "                        newnewToken += ' '\n",
    "                    newnewToken += newToken[i]\n",
    "                    continue\n",
    "                if lc == '.' and cc != ' ':\n",
    "                    newnewToken += ' '\n",
    "                newnewToken += newToken[i]\n",
    "            newToken = newnewToken\n",
    "\n",
    "        if newToken.count('\\'') == 1:\n",
    "            dotidx = newToken.index('\\'')\n",
    "            newToken = rm_bracket(newToken[:dotidx]) + ' \\' ' + rm_bracket(newToken[dotidx + 1:])\n",
    "        if newToken.count(':') == 1:\n",
    "            dotidx = newToken.index(':')\n",
    "            newToken = rm_bracket(newToken[:dotidx]) + ' : ' + rm_bracket(newToken[dotidx + 1:])\n",
    "        for special_entity in ['TCA', 'Cys']:\n",
    "            if special_entity in newToken:  # 特殊对待\n",
    "                idx = newToken.find(special_entity)\n",
    "                if idx + 3 < len(newToken) and newToken[idx + len(special_entity)] != ' ':\n",
    "                    newToken = newToken[:idx] + special_entity + ' ' + newToken[idx + 3:]\n",
    "        newToken = rm_bracket(newToken)\n",
    "        newSent.append(newToken.strip())\n",
    "    newSent = ' '.join(newSent)\n",
    "    while '  ' in newSent:\n",
    "        newSent = newSent.replace('  ', ' ')\n",
    "    return newSent\n",
    "\n",
    "\n",
    "# 11里需要把/ (  )都切开，而13不用\n",
    "def tokenizeCorpus(instances, lower=False, version='13'):\n",
    "    new_instances = []\n",
    "    for target, sentence, d1offset, d2offset in instances:\n",
    "        if type(d1offset[0]) == tuple:\n",
    "            origin_entity1 = sentence[d1offset[0][0]:d1offset[0][1] + 1] + ' ' + sentence[\n",
    "                                                                                 d1offset[1][0]:d1offset[1][1] + 1]\n",
    "        else:\n",
    "            origin_entity1 = sentence[d1offset[0]:d1offset[1] + 1]\n",
    "        if type(d2offset[0]) == tuple:\n",
    "            origin_entity2 = sentence[d2offset[0][0]:d2offset[0][1] + 1] + ' ' + sentence[\n",
    "                                                                                 d2offset[1][0]:d2offset[1][1] + 1]\n",
    "        else:\n",
    "            origin_entity2 = sentence[d2offset[0]:d2offset[1] + 1]\n",
    "\n",
    "        tokened_sentence = extreme_fast_tokenize_sentence(sentence,\n",
    "                                                          special_chars=special_chars_13 if version == '13' else special_chars_11)\n",
    "        new_d1offset = getNewOffset(sentence, tokened_sentence, d1offset)\n",
    "        new_d2offset = getNewOffset(sentence, tokened_sentence, d2offset)\n",
    "\n",
    "        r1 = getIndex(tokened_sentence, new_d1offset)\n",
    "        r2 = getIndex(tokened_sentence, new_d2offset)\n",
    "        new_entity1 = get_entity_by_range(tokened_sentence, r1)\n",
    "        new_entity2 = get_entity_by_range(tokened_sentence, r2)\n",
    "\n",
    "        origin_entity1 = origin_entity1.lower()\n",
    "        origin_entity2 = origin_entity2.lower()\n",
    "        new_entity1 = new_entity1.lower()\n",
    "        new_entity2 = new_entity2.lower()\n",
    "\n",
    "        if origin_entity1 != new_entity1 and new_entity1.replace(' ', '') != origin_entity1.replace(' ', ''):\n",
    "            print('真正实体是：', origin_entity1, d1offset)\n",
    "            print('改变后却是：', new_entity1)\n",
    "            print('出现在', sentence)\n",
    "            print('经过token之后是', tokened_sentence)\n",
    "        if origin_entity2 != new_entity2 and new_entity2.replace(' ', '') != origin_entity2.replace(' ', ''):\n",
    "            print('真正实体是：', origin_entity2, d2offset)\n",
    "            print('改变后却是：', new_entity2)\n",
    "            print('出现在', sentence)\n",
    "            print('经过token之后是', tokened_sentence)\n",
    "\n",
    "        if lower:\n",
    "            tokened_sentence = tokened_sentence.lower()\n",
    "        new_instances.append((target, tokened_sentence, r1, r2))\n",
    "    return new_instances\n",
    "\n",
    "\n",
    "def charset(instances):\n",
    "    char_set = set()\n",
    "    for _, sentence, _, _ in instances:\n",
    "        for c in sentence:\n",
    "            char_set.add(c)\n",
    "    # print(char_set)\n",
    "    return char_set\n",
    "\n",
    "\n",
    "# 如果一个token是由单个特殊符号组成，去掉它\n",
    "def remove_irrelavent_chars(instances):\n",
    "    new_instances = []\n",
    "    special_chars = special_chars_11 + ['.']\n",
    "    for t, sentence, e1r, e2r in instances:\n",
    "        origin_e1 = get_entity_by_range(sentence, e1r)\n",
    "        origin_e2 = get_entity_by_range(sentence, e2r)\n",
    "        token_list = sentence.split(' ')\n",
    "        for i in range(len(token_list) - 1, -1, -1):\n",
    "            if len(token_list[i]) == 1 and token_list[i] in special_chars and i not in e1r and i not in e2r:\n",
    "                e1r = [j - 1 if i < j else j for j in e1r]\n",
    "                e2r = [j - 1 if i < j else j for j in e2r]\n",
    "                token_list.pop(i)\n",
    "        new_sent = ' '.join(token_list)\n",
    "        new_e1 = get_entity_by_range(new_sent, e1r)\n",
    "        new_e2 = get_entity_by_range(new_sent, e2r)\n",
    "        if origin_e1 != new_e1:\n",
    "            print(origin_e1, new_e1, sentence)\n",
    "        if origin_e2 != new_e2:\n",
    "            print(origin_e2, new_e2, sentence)\n",
    "        new_instances.append((t, new_sent, e1r, e2r))\n",
    "    return new_instances\n",
    "\n",
    "\n",
    "def load_gold_11():\n",
    "    for line in open(ddi_2011_gold):\n",
    "        e1, e2, label = line[:-1].split()\n",
    "        DDIE_2011_gold_standard[(e1, e2)] = label\n",
    "\n",
    "\n",
    "def xml2plain(xmlfolders, plain_file, replace_drug=True, remove_special_chars=False, version='13'):\n",
    "    if version == '11':\n",
    "        load_gold_11()\n",
    "    if isinstance(xmlfolders, str):\n",
    "        xmlfolders = [xmlfolders]\n",
    "    instances = []\n",
    "    bw = open(plain_file, 'w')\n",
    "    for folder in xmlfolders:\n",
    "        if folder.endswith('/'):\n",
    "            folder = folder[:-1]\n",
    "        for filename in os.listdir(folder):\n",
    "            instances.extend(get_instances_from_xml_file(folder + '/' + filename, version=version))\n",
    "    print(u\"在tokenization之前，instances的数目是：\", len(instances))\n",
    "\n",
    "    instances = tokenizeCorpus(instances, version=version)\n",
    "    if replace_drug:\n",
    "        instances = replace_irrelavent_drugs(instances)\n",
    "    if remove_special_chars:\n",
    "        instances = remove_irrelavent_chars(instances)\n",
    "\n",
    "    print(u\"在tokenization之后，instances的数目是：\", len(instances))\n",
    "    for instance in instances:\n",
    "        bw.write('\\t'.join([str(ele) for ele in instance]) + '\\n')\n",
    "    bw.close()\n",
    "    print(u\"写入文件之后，instances的数目是：\", file_len(plain_file))\n",
    "\n",
    "\n",
    "# xml2plain([ddi_db_2013_train_xml_folder], ddi_2013_DB_train, version='13')\n",
    "# xml2plain([ddi_ml_2013_train_xml_folder], ddi_2013_ML_train, version='13')\n",
    "# xml2plain([ddi_db_2013_test_xml_folder], ddi_2013_DB_test, version='13')\n",
    "# xml2plain([ddi_ml_2013_test_xml_folder], ddi_2013_ML_test, version='13')\n",
    "#\n",
    "# xml2plain([ddi_2011_train_xml_folder], ddi_2011_train, version='11')\n",
    "# xml2plain([ddi_2011_test_xml_folder], ddi_2011_test, version='11')\n",
    "\n",
    "# extract_sentence([ddi_2011_train, ddi_2011_test], ddi_2011_sentences)\n",
    "# extract_sentence([ddi_2013_DB_train, ddi_2013_ML_train, ddi_2013_DB_test, ddi_2013_ML_test], ddi_2013_sentences)\n",
    "\n",
    "all_data = load_data()\n",
    "# summarize_ddi_instance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#callback部分\n",
    "\n",
    "import keras\n",
    "import time\n",
    "from sklearn.metrics.classification import  confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n",
    "class evaluator_2013(keras.callbacks.Callback):\n",
    "    def __init__(self, model, X_test, X_test_distance1, X_test_distance2, X_test_skeleton, y_test, dim):\n",
    "        self.model = model\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.X_test_distance1 = X_test_distance1\n",
    "        self.X_test_distance2 = X_test_distance2\n",
    "        self.X_test_skeleton = X_test_skeleton\n",
    "        self.dim = dim\n",
    "\n",
    "\n",
    "    def show_DDIE_result(self, y_true, y_pred):\n",
    "        def precision(TP, FP):\n",
    "            if TP == FP == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return TP / (TP + FP)\n",
    "\n",
    "        def recall(TP, FN):\n",
    "            if TP == FN == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return TP / (TP + FN)\n",
    "\n",
    "        def f_score(p, r):\n",
    "            if p == r == 0.0:\n",
    "                return 0.0\n",
    "            else:\n",
    "                return 2 * p * r / (p + r)\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        cm_tp_fp_fn = []\n",
    "        result = [[], [], []]\n",
    "        for i in range(1, len(cm)):\n",
    "            tp = cm[i][i]\n",
    "            fp = sum(cm[:, i]) - tp\n",
    "            fn = sum(cm[i, :]) - tp\n",
    "            cm_tp_fp_fn.append((tp, fp, fn))\n",
    "        micro_p = precision(sum([x[0] for x in cm_tp_fp_fn]), sum([x[1] for x in cm_tp_fp_fn]))\n",
    "        micro_r = recall(sum([x[0] for x in cm_tp_fp_fn]), sum([x[2] for x in cm_tp_fp_fn]))\n",
    "        micro_f = f_score(micro_p, micro_r)\n",
    "        result[0].append(micro_p)\n",
    "        result[1].append(micro_r)\n",
    "        result[2].append(micro_f)\n",
    "        binary_p, binary_r, binary_f, _ = precision_recall_fscore_support(y_true != 0, y_pred != 0, )\n",
    "        result[0].append(binary_p[1])\n",
    "        result[1].append(binary_r[1])\n",
    "        result[2].append(binary_f[1])\n",
    "        ps = [precision(x[0], x[1]) for x in cm_tp_fp_fn]\n",
    "        rs = [recall(x[0], x[2]) for x in cm_tp_fp_fn]\n",
    "        fs = [f_score(ps[i], rs[i]) for i in range(len(ps))]\n",
    "        for i in range(len(ps)):\n",
    "            result[0].append(ps[i])\n",
    "            result[1].append(rs[i])\n",
    "            result[2].append(fs[i])\n",
    "        avg_p = sum(ps) / len(ps)\n",
    "        avg_r = sum(rs) / len(rs)\n",
    "        result[0].append(avg_p)\n",
    "        result[1].append(avg_r)\n",
    "        mavg_f = f_score(avg_p, avg_r)\n",
    "        result[2].append(mavg_f)\n",
    "        print(\"\\t\\t\".join(['CLA', 'DEC', 'MEC', 'EFF', 'ADV', 'INT', 'MAVG']))\n",
    "        for r in result:\n",
    "            print('\\t'.join([\"%.3f\" % v for v in r]))\n",
    "        return mavg_f\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.times = []\n",
    "        self.mafscores = []\n",
    "        self.dbfscores = []\n",
    "        self.mlfscores = []\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "        # Once the training has ended, display the animation\n",
    "        y_pred = self.model.predict(\n",
    "            [self.X_test, self.X_test_distance1, self.X_test_distance2, self.X_test_skeleton]).argmax(axis=1)\n",
    "\n",
    "        #\n",
    "        y_true = self.y_test.argmax(axis=1)\n",
    "\n",
    "        assert (len(y_pred) == 5265 + 451)\n",
    "        y_pred_DB = y_pred[:5265]\n",
    "        y_pred_ML = y_pred[5265:]\n",
    "        y_true_DB = y_true[:5265]\n",
    "        y_true_ML = y_true[5265:]\n",
    "        print()\n",
    "        print(\"The {}th epoch\".format(epoch))\n",
    "        print(\"------------------Drug Bank Result------------------\")\n",
    "        DBf = self.show_DDIE_result(y_true_DB, y_pred_DB)\n",
    "        print(\"-------------------MEDLINE Result-------------------\")\n",
    "        MLf = self.show_DDIE_result(y_true_ML, y_pred_ML)\n",
    "        print(\"-------------------All Set Result-------------------\")\n",
    "        MAf = self.show_DDIE_result(y_true, y_pred)\n",
    "        # self.model.save_weights('13_weights_dim_' + str(self.dim) + '_epoch_' + str(epoch))\n",
    "        self.epoch_time_start = time.time()\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.mafscores.append(MAf)\n",
    "        self.dbfscores.append(MLf)\n",
    "        self.mlfscores.append(DBf)\n",
    "\n",
    "\n",
    "class evaluator_2011(keras.callbacks.Callback):\n",
    "    def __init__(self, model, X_test, X_test_distance1, X_test_distance2, X_test_skeleton, y_test, dim):\n",
    "        self.model = model\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.X_test_distance1 = X_test_distance1\n",
    "        self.X_test_distance2 = X_test_distance2\n",
    "        self.X_test_skeleton = X_test_skeleton\n",
    "        self.dim = dim\n",
    "        self.iter = iter\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.times = []\n",
    "        self.fscores = []\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "        # Once the training has ended, display the animation\n",
    "        y_pred = self.model.predict(\n",
    "            [self.X_test, self.X_test_distance1, self.X_test_distance2, self.X_test_skeleton]).argmax(axis=1)\n",
    "        #\n",
    "        y_true = self.y_test.argmax(axis=1)\n",
    "        print()\n",
    "        print(\"The {}th epoch\".format(epoch))\n",
    "        print(\"------------------DDIE2011 Result------------------\")\n",
    "        # y_true = y_true != 0\n",
    "        # y_pred = y_pred != 0\n",
    "        print(y_true, sum(y_true))\n",
    "        print(y_pred, sum(y_pred))\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        TP = cm[1][1]\n",
    "        FP = cm[0][1]\n",
    "        TN = cm[0][0]\n",
    "        FN = cm[1][0]\n",
    "        binary_p, binary_r, binary_f, _ = precision_recall_fscore_support(y_true, y_pred)\n",
    "        P = binary_p[1]\n",
    "        R = binary_r[1]\n",
    "        F = binary_f[1]\n",
    "        Acc = accuracy_score(y_true, y_pred)\n",
    "        print(\"\\t\\t\".join(['TP', 'FP', 'FN', 'TN', 'P', 'R', 'F', 'Acc']))\n",
    "        print(\"\\t\".join(\n",
    "            [str(x) + ('\\t' if x < 1000 else \"\") for x in [TP, FP, FN, TN]] + [\"%.4f\" % v for v in [P, R, F, Acc]]))\n",
    "        # self.model.save_weights('11_weights_dim_' + str(self.dim) + '_epoch_' + str(epoch))\n",
    "\n",
    "\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.fscores.append(F)\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "X_train shape: (23827, 131)\n",
      "X_test shape: (7026, 110)\n",
      "Build model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_37 (Embedding)         (None, None, 400)     3624400                                      \n",
      "____________________________________________________________________________________________________\n",
      "embedding_38 (Embedding)         (None, None, 50)      12800                                        \n",
      "____________________________________________________________________________________________________\n",
      "embedding_39 (Embedding)         (None, None, 50)      12800                                        \n",
      "____________________________________________________________________________________________________\n",
      "embedding_40 (Embedding)         (None, None, 50)      300                                          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                   (None, 400)           1521600     merge_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 2)             802         lstm_10[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 2)             0           dense_8[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 5,172,702\n",
      "Trainable params: 5,172,702\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\workbench\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "X_train shape: (1787, 94)\n",
      "X_test shape: (5716, 109)\n",
      "Build model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_41 (Embedding)         (None, None, 400)     3624400                                      \n",
      "____________________________________________________________________________________________________\n",
      "embedding_42 (Embedding)         (None, None, 50)      12800                                        \n",
      "____________________________________________________________________________________________________\n",
      "embedding_43 (Embedding)         (None, None, 50)      12800                                        \n",
      "____________________________________________________________________________________________________\n",
      "embedding_44 (Embedding)         (None, None, 50)      300                                          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                   (None, 400)           1521600     merge_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 5)             2005        lstm_11[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 5)             0           dense_9[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 5,173,905\n",
      "Trainable params: 5,173,905\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train...\n"
     ]
    }
   ],
   "source": [
    "# 预训练部分\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, LSTM\n",
    "from keras.engine.topology import Merge\n",
    "np.random.seed(2)\n",
    "maxlen = None\n",
    "dim = 400\n",
    "batch_size = 32\n",
    "dropout_ratio = 0.2\n",
    "start_seed = 0\n",
    "flag = False\n",
    "def pretrain(X_train, Y_train, X_test, Y_test, word_embedding_size, dis_embedding_size, version='13'):\n",
    "    X_train_text, X_train_distance1, X_train_distance2, X_train_skeleton, X_train_inner_tokens, X_train_surrounding_tokens, X_train_e1, X_train_e2 = X_train\n",
    "    X_test_text, X_test_distance1, X_test_distance2, X_test_skeleton, X_test_inner_tokens, X_test_surrounding_tokens, X_test_e1, X_test_e2 = X_test\n",
    "\n",
    "    print('Pad sequences (samples x time)')\n",
    "    X_train_text = sequence.pad_sequences(X_train_text, maxlen=maxlen)\n",
    "    X_train_inner_tokens = sequence.pad_sequences(X_train_inner_tokens, maxlen=maxlen)\n",
    "    X_test_text = sequence.pad_sequences(X_test_text, maxlen=maxlen)\n",
    "    X_test_inner_tokens = sequence.pad_sequences(X_test_inner_tokens, maxlen=maxlen)\n",
    "    X_train_distance1 = sequence.pad_sequences(X_train_distance1, maxlen=maxlen)\n",
    "    X_train_distance2 = sequence.pad_sequences(X_train_distance2, maxlen=maxlen)\n",
    "    X_train_skeleton = sequence.pad_sequences(X_train_skeleton, maxlen=maxlen)\n",
    "    X_test_distance1 = sequence.pad_sequences(X_test_distance1, maxlen=maxlen)\n",
    "    X_test_distance2 = sequence.pad_sequences(X_test_distance2, maxlen=maxlen)\n",
    "    X_test_skeleton = sequence.pad_sequences(X_test_skeleton, maxlen=maxlen)\n",
    "    print('X_train shape:', X_train_text.shape)\n",
    "    print('X_test shape:', X_test_text.shape)\n",
    "    # for dim in [300, 400, 500]:\n",
    "    #     for iter in range(2):\n",
    "    print('Build model...')\n",
    "    \n",
    "\n",
    "    word_input = Sequential()\n",
    "    word_input.add(\n",
    "        Embedding(word_embedding_size, EMBEDDING_DIM, dropout=dropout_ratio, trainable=True))\n",
    "\n",
    "    dis1_input = Sequential()\n",
    "    dis1_input.add(Embedding(dis_embedding_size, 50, dropout=dropout_ratio))\n",
    "\n",
    "    dis2_input = Sequential()\n",
    "    dis2_input.add(Embedding(dis_embedding_size, 50, dropout=dropout_ratio))\n",
    "\n",
    "    skeleton = Sequential()\n",
    "    skeleton.add(Embedding(6, 50, dropout=dropout_ratio))\n",
    "    model = Sequential()\n",
    "    # model.add(word_input)\n",
    "    # model.add(merge([word_input, dis1_input, dis2_input, skeleton], mode='concat'))\n",
    "\n",
    "    model.add(Merge([word_input, dis1_input, dis2_input, skeleton], mode='concat'))\n",
    "    model.add(LSTM(dim, dropout_W=dropout_ratio, dropout_U=dropout_ratio))\n",
    "\n",
    "    if version == '13':\n",
    "        model.add(Dense(5))\n",
    "\n",
    "    elif version == '11':\n",
    "        model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop') #sgd\n",
    "    model.summary()\n",
    "    print('Train...')\n",
    "    \n",
    "    if version == '13':\n",
    "        evlt = evaluator_2013(model, X_test_text, X_test_distance1, X_test_distance2, X_test_skeleton, Y_test, dim)\n",
    "    elif version == '11':\n",
    "        evlt = evaluator_2011(model, X_test_text, X_test_distance1, X_test_distance2, X_test_skeleton, Y_test, dim)\n",
    "        \n",
    "    model.fit([X_train_text, X_train_distance1, X_train_distance2, X_train_skeleton], Y_train, batch_size=batch_size,\n",
    "              nb_epoch=20,\n",
    "              callbacks=[evlt], verbose=0)\n",
    "    return model\n",
    "\n",
    "data2011, data2013, word_embedding_size, dis_embedding_size = all_data\n",
    "X_train2011, Y_train2011, X_test2011, Y_test2011 = data2011\n",
    "X_train2013, Y_train2013, X_test2013, Y_test2013 = data2013\n",
    "pretrained_model_11 = pretrain(X_train2011, Y_train2011, X_test2011, Y_test2011,  word_embedding_size, dis_embedding_size, version='11')\n",
    "pretrained_model_13 = pretrain(X_train2013, Y_train2013, X_test2013, Y_test2013,  word_embedding_size, dis_embedding_size, version='13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "X_train shape: (23827, 131)\n",
      "X_test shape: (7026, 110)\n",
      "Build model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_49 (Embedding)         (None, None, 400)     3624400                                      \n",
      "____________________________________________________________________________________________________\n",
      "embedding_50 (Embedding)         (None, None, 50)      12800                                        \n",
      "____________________________________________________________________________________________________\n",
      "embedding_51 (Embedding)         (None, None, 50)      12800                                        \n",
      "____________________________________________________________________________________________________\n",
      "embedding_52 (Embedding)         (None, None, 50)      300                                          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                   (None, 400)           1521600     merge_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 2)             802         lstm_13[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 2)             0           dense_10[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 5,172,702\n",
      "Trainable params: 5,172,702\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\workbench\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 0th epoch\n",
      "------------------DDIE2011 Result------------------\n",
      "[1 0 1 ... 0 0 0] 755\n",
      "[0 0 0 ... 0 0 0] 10\n",
      "TP\t\tFP\t\tFN\t\tTN\t\tP\t\tR\t\tF\t\tAcc\n",
      "7\t\t3\t\t748\t\t6268\t0.7000\t0.0093\t0.0183\t0.8931\n",
      "\n",
      "The 1th epoch\n",
      "------------------DDIE2011 Result------------------\n",
      "[1 0 1 ... 0 0 0] 755\n",
      "[0 0 0 ... 0 0 0] 355\n",
      "TP\t\tFP\t\tFN\t\tTN\t\tP\t\tR\t\tF\t\tAcc\n",
      "236\t\t119\t\t519\t\t6152\t0.6648\t0.3126\t0.4252\t0.9092\n",
      "\n",
      "The 2th epoch\n",
      "------------------DDIE2011 Result------------------\n",
      "[1 0 1 ... 0 0 0] 755\n",
      "[0 0 0 ... 0 0 0] 633\n",
      "TP\t\tFP\t\tFN\t\tTN\t\tP\t\tR\t\tF\t\tAcc\n",
      "366\t\t267\t\t389\t\t6004\t0.5782\t0.4848\t0.5274\t0.9066\n",
      "\n",
      "The 3th epoch\n",
      "------------------DDIE2011 Result------------------\n",
      "[1 0 1 ... 0 0 0] 755\n",
      "[0 0 0 ... 0 0 0] 367\n",
      "TP\t\tFP\t\tFN\t\tTN\t\tP\t\tR\t\tF\t\tAcc\n",
      "261\t\t106\t\t494\t\t6165\t0.7112\t0.3457\t0.4652\t0.9146\n",
      "\n",
      "The 4th epoch\n",
      "------------------DDIE2011 Result------------------\n",
      "[1 0 1 ... 0 0 0] 755\n",
      "[0 0 0 ... 0 0 0] 977\n",
      "TP\t\tFP\t\tFN\t\tTN\t\tP\t\tR\t\tF\t\tAcc\n",
      "504\t\t473\t\t251\t\t5798\t0.5159\t0.6675\t0.5820\t0.8970\n",
      "\n",
      "The 5th epoch\n",
      "------------------DDIE2011 Result------------------\n",
      "[1 0 1 ... 0 0 0] 755\n",
      "[0 0 0 ... 0 0 0] 1021\n",
      "TP\t\tFP\t\tFN\t\tTN\t\tP\t\tR\t\tF\t\tAcc\n",
      "538\t\t483\t\t217\t\t5788\t0.5269\t0.7126\t0.6059\t0.9004\n",
      "\n",
      "The 6th epoch\n",
      "------------------DDIE2011 Result------------------\n",
      "[1 0 1 ... 0 0 0] 755\n",
      "[0 0 0 ... 0 0 0] 882\n",
      "TP\t\tFP\t\tFN\t\tTN\t\tP\t\tR\t\tF\t\tAcc\n",
      "505\t\t377\t\t250\t\t5894\t0.5726\t0.6689\t0.6170\t0.9108\n",
      "\n",
      "The 7th epoch\n",
      "------------------DDIE2011 Result------------------\n",
      "[1 0 1 ... 0 0 0] 755\n",
      "[1 0 0 ... 0 0 0] 1008\n",
      "TP\t\tFP\t\tFN\t\tTN\t\tP\t\tR\t\tF\t\tAcc\n",
      "545\t\t463\t\t210\t\t5808\t0.5407\t0.7219\t0.6183\t0.9042\n",
      "\n",
      "The 8th epoch\n",
      "------------------DDIE2011 Result------------------\n",
      "[1 0 1 ... 0 0 0] 755\n",
      "[0 0 0 ... 0 0 0] 804\n",
      "TP\t\tFP\t\tFN\t\tTN\t\tP\t\tR\t\tF\t\tAcc\n",
      "477\t\t327\t\t278\t\t5944\t0.5933\t0.6318\t0.6119\t0.9139\n",
      "\n",
      "The 9th epoch\n",
      "------------------DDIE2011 Result------------------\n",
      "[1 0 1 ... 0 0 0] 755\n",
      "[0 0 0 ... 0 0 0] 1109\n",
      "TP\t\tFP\t\tFN\t\tTN\t\tP\t\tR\t\tF\t\tAcc\n",
      "576\t\t533\t\t179\t\t5738\t0.5194\t0.7629\t0.6180\t0.8987\n",
      "\n",
      "The 10th epoch\n",
      "------------------DDIE2011 Result------------------\n",
      "[1 0 1 ... 0 0 0] 755\n",
      "[0 0 0 ... 0 0 0] 723\n",
      "TP\t\tFP\t\tFN\t\tTN\t\tP\t\tR\t\tF\t\tAcc\n",
      "467\t\t256\t\t288\t\t6015\t0.6459\t0.6185\t0.6319\t0.9226\n",
      "\n",
      "The 11th epoch\n",
      "------------------DDIE2011 Result------------------\n",
      "[1 0 1 ... 0 0 0] 755\n",
      "[0 0 0 ... 0 0 0] 611\n",
      "TP\t\tFP\t\tFN\t\tTN\t\tP\t\tR\t\tF\t\tAcc\n",
      "420\t\t191\t\t335\t\t6080\t0.6874\t0.5563\t0.6149\t0.9251\n",
      "\n",
      "The 12th epoch\n",
      "------------------DDIE2011 Result------------------\n",
      "[1 0 1 ... 0 0 0] 755\n",
      "[0 0 0 ... 0 0 0] 594\n",
      "TP\t\tFP\t\tFN\t\tTN\t\tP\t\tR\t\tF\t\tAcc\n",
      "409\t\t185\t\t346\t\t6086\t0.6886\t0.5417\t0.6064\t0.9244\n",
      "\n",
      "The 13th epoch\n",
      "------------------DDIE2011 Result------------------\n",
      "[1 0 1 ... 0 0 0] 755\n",
      "[0 0 0 ... 0 0 0] 621\n",
      "TP\t\tFP\t\tFN\t\tTN\t\tP\t\tR\t\tF\t\tAcc\n",
      "421\t\t200\t\t334\t\t6071\t0.6779\t0.5576\t0.6119\t0.9240\n"
     ]
    }
   ],
   "source": [
    "def train(X_train, Y_train, X_test, Y_test, word_embedding_size, dis_embedding_size,  pretrained_model, version='13'):\n",
    "    X_train_text, X_train_distance1, X_train_distance2, X_train_skeleton, X_train_inner_tokens, X_train_surrounding_tokens, X_train_e1, X_train_e2 = X_train\n",
    "    X_test_text, X_test_distance1, X_test_distance2, X_test_skeleton, X_test_inner_tokens, X_test_surrounding_tokens, X_test_e1, X_test_e2 = X_test\n",
    "\n",
    "    print('Pad sequences (samples x time)')\n",
    "    X_train_text = sequence.pad_sequences(X_train_text, maxlen=maxlen)\n",
    "    X_train_inner_tokens = sequence.pad_sequences(X_train_inner_tokens, maxlen=maxlen)\n",
    "    X_test_text = sequence.pad_sequences(X_test_text, maxlen=maxlen)\n",
    "    X_test_inner_tokens = sequence.pad_sequences(X_test_inner_tokens, maxlen=maxlen)\n",
    "    X_train_distance1 = sequence.pad_sequences(X_train_distance1, maxlen=maxlen)\n",
    "    X_train_distance2 = sequence.pad_sequences(X_train_distance2, maxlen=maxlen)\n",
    "    X_train_skeleton = sequence.pad_sequences(X_train_skeleton, maxlen=maxlen)\n",
    "    X_test_distance1 = sequence.pad_sequences(X_test_distance1, maxlen=maxlen)\n",
    "    X_test_distance2 = sequence.pad_sequences(X_test_distance2, maxlen=maxlen)\n",
    "    X_test_skeleton = sequence.pad_sequences(X_test_skeleton, maxlen=maxlen)\n",
    "    print('X_train shape:', X_train_text.shape)\n",
    "    print('X_test shape:', X_test_text.shape)\n",
    "    # for dim in [300, 400, 500]:\n",
    "    #     for iter in range(2):\n",
    "    print('Build model...')\n",
    "    \n",
    "\n",
    "    word_input = Sequential()\n",
    "    word_input.add(\n",
    "        Embedding(word_embedding_size, EMBEDDING_DIM, dropout=dropout_ratio, trainable=True))\n",
    "\n",
    "    dis1_input = Sequential()\n",
    "    dis1_input.add(Embedding(dis_embedding_size, 50, dropout=dropout_ratio))\n",
    "\n",
    "    dis2_input = Sequential()\n",
    "    dis2_input.add(Embedding(dis_embedding_size, 50, dropout=dropout_ratio))\n",
    "\n",
    "    skeleton = Sequential()\n",
    "    skeleton.add(Embedding(6, 50, dropout=dropout_ratio))\n",
    "    model = Sequential()\n",
    "    # model.add(word_input)\n",
    "    # model.add(merge([word_input, dis1_input, dis2_input, skeleton], mode='concat'))\n",
    "\n",
    "    model.add(Merge([word_input, dis1_input, dis2_input, skeleton], mode='concat'))\n",
    "    model.add(LSTM(dim, dropout_W=dropout_ratio, dropout_U=dropout_ratio))\n",
    "    model.layers[0].set_weights(pretrained_model.layers[0].get_weights())\n",
    "    model.layers[1].set_weights(pretrained_model.layers[1].get_weights())\n",
    "    if version == '13':\n",
    "        model.add(Dense(5))\n",
    "\n",
    "    elif version == '11':\n",
    "        model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop') #sgd\n",
    "    model.summary()\n",
    "    print('Train...')\n",
    "    \n",
    "    if version == '13':\n",
    "        evlt = evaluator_2013(model, X_test_text, X_test_distance1, X_test_distance2, X_test_skeleton, Y_test, dim)\n",
    "    elif version == '11':\n",
    "        evlt = evaluator_2011(model, X_test_text, X_test_distance1, X_test_distance2, X_test_skeleton, Y_test, dim)\n",
    "        \n",
    "    model.fit([X_train_text, X_train_distance1, X_train_distance2, X_train_skeleton], Y_train, batch_size=batch_size,\n",
    "              nb_epoch=20,\n",
    "              callbacks=[evlt], verbose=0)\n",
    "    return model\n",
    "\n",
    "train(X_train2011, Y_train2011, X_test2011, Y_test2011,  word_embedding_size, dis_embedding_size,pretrained_model_13,  version='11')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
